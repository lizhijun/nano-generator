# LLaMA-Factory / Unsloth 训练配置

# 模型配置
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
trust_remote_code: true

# LoRA 配置
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target: all  # 或指定: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

# 训练配置
num_train_epochs: 3
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
learning_rate: 2.0e-4
lr_scheduler_type: cosine
warmup_ratio: 0.1
max_grad_norm: 1.0

# 数据配置
max_seq_length: 2048
preprocessing_num_workers: 4

# 优化器配置
optim: adamw_torch
weight_decay: 0.01

# 日志和保存
logging_steps: 10
save_steps: 100
save_total_limit: 3
output_dir: ./output/nano-banana-lora

# 评估配置
eval_strategy: steps
eval_steps: 100

# 其他
bf16: true  # Mac M系列使用 bf16
seed: 42
